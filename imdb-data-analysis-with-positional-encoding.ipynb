{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training and Evaluating Sentiment Analysis on IMDB Dataset with PyTorch and Positional Encoding","metadata":{}},{"cell_type":"markdown","source":"Please find the dataset here: [IMDB Dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n\n\n50K movie reviews from the IMDB dataset are available for text analytics or natural language processing. Compared to earlier benchmark datasets, this dataset for binary sentiment classification has a significantly larger amount of data. For training, it offers a set of 25,000 highly polar movie reviews, and for testing, it offers another 25,000.","metadata":{}},{"cell_type":"markdown","source":"## Importing Required Packages","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchtext.datasets import IMDB                       # Import IMDB dataset from torchtext\nfrom torchtext.data.utils import get_tokenizer            # Import tokenizer utility from torchtext\nfrom torchtext.vocab import build_vocab_from_iterator     # Import vocabulary building function from torchtext.vocab\nfrom torchtext.vocab import Vocab                         # Import Vocab class from torchtext.vocab\nfrom torch.utils.data import DataLoader                   # Import DataLoader from torch.utils.data\nfrom torch.nn.utils.rnn import pad_sequence               # Import pad_sequence utility from torch.nn.utils.rnn\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:55:09.886503Z","iopub.execute_input":"2024-05-06T15:55:09.887192Z","iopub.status.idle":"2024-05-06T15:55:12.812951Z","shell.execute_reply.started":"2024-05-06T15:55:09.887161Z","shell.execute_reply":"2024-05-06T15:55:12.812195Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Creating Vocabulary","metadata":{}},{"cell_type":"markdown","source":"* **Tokenization and Download**: To prepare the text data, it first downloads the IMDB dataset and sets up a simple English tokenizer. \n* **Build a vocabulary**: tokenized sequences from the IMDB dataset are assembled using the build_vocab_from_iterator function.","metadata":{}},{"cell_type":"code","source":"# Download and tokenize IMDB dataset\ntokenizer = get_tokenizer('basic_english')\n\ndef yield_tokens(data_iter):\n    for _, text in data_iter:\n        yield tokenizer(text)\n\n# Create vocabulary\ntrain_iter = IMDB(split='train')\nvocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\", \"<pad>\"])\nvocab.set_default_index(vocab[\"<unk>\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:55:12.814783Z","iopub.execute_input":"2024-05-06T15:55:12.815329Z","iopub.status.idle":"2024-05-06T15:55:19.020140Z","shell.execute_reply.started":"2024-05-06T15:55:12.815295Z","shell.execute_reply":"2024-05-06T15:55:19.019326Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Iterate through the dataset to get its length\ndataset_length = sum(1 for _ in train_iter)\nprint(\"Length of the dataset:\", dataset_length)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:55:19.808309Z","iopub.execute_input":"2024-05-06T15:55:19.808672Z","iopub.status.idle":"2024-05-06T15:55:20.594976Z","shell.execute_reply.started":"2024-05-06T15:55:19.808640Z","shell.execute_reply":"2024-05-06T15:55:20.593973Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Length of the dataset: 25000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Text Data Processing","metadata":{}},{"cell_type":"markdown","source":"The below functions defined help to prepare the data for training by converting text and label inputs into numerical representations, ensuring consistent sequence lengths within batches, and structuring the data in a way that is compatible with inputting into a neural network.","metadata":{}},{"cell_type":"code","source":"# Text data processing function\ndef text_pipeline(text):\n    return [vocab[token] for token in tokenizer(text)]\n\n# Label data processing function\ndef label_pipeline(label):\n    return 1 if label == 'pos' else 0\n\n# Truncate sequences that exceed MAX_SEQ_LEN\nMAX_SEQ_LEN = 2048\n\ndef truncate_sequence(seq):\n    return seq[:MAX_SEQ_LEN]\n\n# Collate function to process batch data\ndef collate_batch(batch):\n    label_list, text_list = [], []\n    for _label, _text in batch:\n        label_list.append(torch.tensor(label_pipeline(_label), dtype=torch.long))\n        processed_text = torch.tensor(truncate_sequence(text_pipeline(_text)), dtype=torch.long)\n        text_list.append(processed_text)\n    text_list = pad_sequence(text_list, padding_value=vocab[\"<pad>\"])\n    return torch.transpose(text_list, 0, 1), torch.stack(label_list)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:55:20.596010Z","iopub.execute_input":"2024-05-06T15:55:20.596286Z","iopub.status.idle":"2024-05-06T15:55:20.604100Z","shell.execute_reply.started":"2024-05-06T15:55:20.596260Z","shell.execute_reply":"2024-05-06T15:55:20.603255Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Positional Encoding","metadata":{}},{"cell_type":"markdown","source":"This code enables the model to capture sequential information in a parameterized way by dynamically appending learnable positional encodings to input sequences.","metadata":{}},{"cell_type":"code","source":"# Learnable positional encoding module\nclass PositionalEncoding(nn.Module):\n    def __init__(self, max_seq_len, embedding_dim):\n        super(PositionalEncoding, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.positional_encoding = nn.Parameter(torch.randn(max_seq_len, embedding_dim))\n        \n    def forward(self, x):\n        seq_len = x.size(1)\n        # Limit the positional encoding to the current sequence length\n        positional_encoding = self.positional_encoding[:seq_len, :].unsqueeze(0).repeat(x.size(0), 1, 1)\n        # Add positional encoding to the input\n        x = x + positional_encoding\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:55:20.605474Z","iopub.execute_input":"2024-05-06T15:55:20.606112Z","iopub.status.idle":"2024-05-06T15:55:20.614662Z","shell.execute_reply.started":"2024-05-06T15:55:20.606079Z","shell.execute_reply":"2024-05-06T15:55:20.613921Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Model Architecture","metadata":{}},{"cell_type":"code","source":"# Model\nclass Model(nn.Module):\n    def __init__(self, vocab_size, input_dim, hidden_dim, max_seq_len):\n        super(Model, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, input_dim, padding_idx=vocab[\"<pad>\"])\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.rnn = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 2)  # Binary classification (pos/neg)\n        self.positional_encoding = PositionalEncoding(max_seq_len=max_seq_len, embedding_dim=input_dim)\n    \n    def forward(self, x):\n        # Embedding\n        x = self.embedding(x)\n        # Add positional encoding\n        x = self.positional_encoding(x)\n        # RNN forward pass\n        out, _ = self.rnn(x)\n        # Linear layer\n        out = self.linear(out[:, -1, :])\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:55:20.615791Z","iopub.execute_input":"2024-05-06T15:55:20.616066Z","iopub.status.idle":"2024-05-06T15:55:20.626611Z","shell.execute_reply.started":"2024-05-06T15:55:20.616042Z","shell.execute_reply":"2024-05-06T15:55:20.625732Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# Training function\ndef train(model, criterion, optimizer, dataloader, device, epochs=10):\n    model.train()\n    model.to(device)\n    for epoch in range(epochs):\n        epoch_loss = 0\n        num_batches = 0  # Count the number of batches\n        for data, target in dataloader:\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            # Forward pass through the model\n            output = model(data)\n            # Compute the loss\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n            num_batches += 1\n        avg_loss = epoch_loss / num_batches\n        print(f\"Epoch {epoch + 1}, Loss: {avg_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:55:20.627752Z","iopub.execute_input":"2024-05-06T15:55:20.628391Z","iopub.status.idle":"2024-05-06T15:55:20.641783Z","shell.execute_reply.started":"2024-05-06T15:55:20.628366Z","shell.execute_reply":"2024-05-06T15:55:20.641043Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluation of the model\ndef evaluate(model, criterion, dataloader, device):\n    model.eval()\n    model.to(device)\n    total_loss = 0\n    correct = 0\n    num_batches = 0\n    with torch.no_grad():\n        for data, target in dataloader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            loss = criterion(output, target)\n            total_loss += loss.item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            num_batches += 1\n    avg_loss = total_loss / num_batches\n    accuracy = correct / len(dataloader.dataset)\n    return avg_loss, accuracy","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:55:20.644654Z","iopub.execute_input":"2024-05-06T15:55:20.645199Z","iopub.status.idle":"2024-05-06T15:55:20.653034Z","shell.execute_reply.started":"2024-05-06T15:55:20.645173Z","shell.execute_reply":"2024-05-06T15:55:20.652300Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"For training and testing purposes, this class offers an interface to access samples from the IMDB dataset.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass IMDBDataset(Dataset):\n    def __init__(self, split='train'):\n        self.dataset = list(IMDB(split=split))  \n\n    def __getitem__(self, index):\n        return self.dataset[index]\n\n    def __len__(self):\n        return len(self.dataset)\n\n\ntrain_dataset = IMDBDataset(split='train')\ntest_dataset = IMDBDataset(split='test')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:56:12.441469Z","iopub.execute_input":"2024-05-06T15:56:12.442107Z","iopub.status.idle":"2024-05-06T15:56:14.047816Z","shell.execute_reply.started":"2024-05-06T15:56:12.442074Z","shell.execute_reply":"2024-05-06T15:56:14.047011Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters\nBATCH_SIZE = 32\nINPUT_DIM = 128  # Dimension of input features/embeddings\nHIDDEN_DIM = 64\nLEARNING_RATE = 0.001\nEPOCHS = 5","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:55:20.675674Z","iopub.execute_input":"2024-05-06T15:55:20.676400Z","iopub.status.idle":"2024-05-06T15:55:20.684869Z","shell.execute_reply.started":"2024-05-06T15:55:20.676375Z","shell.execute_reply":"2024-05-06T15:55:20.684170Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Set device (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Create dataloaders for the IMDB dataset\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:56:16.576057Z","iopub.execute_input":"2024-05-06T15:56:16.576737Z","iopub.status.idle":"2024-05-06T15:56:16.582115Z","shell.execute_reply.started":"2024-05-06T15:56:16.576698Z","shell.execute_reply":"2024-05-06T15:56:16.581194Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Create model, loss function, and optimizer\nmodel = Model(vocab_size=len(vocab), input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, max_seq_len=MAX_SEQ_LEN)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:56:18.622017Z","iopub.execute_input":"2024-05-06T15:56:18.623015Z","iopub.status.idle":"2024-05-06T15:56:19.788513Z","shell.execute_reply.started":"2024-05-06T15:56:18.622979Z","shell.execute_reply":"2024-05-06T15:56:19.787584Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Train the model\ntrain(model, criterion, optimizer, train_dataloader, device, epochs=EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:56:19.790327Z","iopub.execute_input":"2024-05-06T15:56:19.791266Z","iopub.status.idle":"2024-05-06T15:57:58.669986Z","shell.execute_reply.started":"2024-05-06T15:56:19.791225Z","shell.execute_reply":"2024-05-06T15:57:58.668915Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.051114533210790014\nEpoch 2, Loss: 0.00011741973729470692\nEpoch 3, Loss: 5.1647593840629775e-05\nEpoch 4, Loss: 2.799364862890388e-05\nEpoch 5, Loss: 1.6617554856821998e-05\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate the model\ntest_loss, test_accuracy = evaluate(model, criterion, test_dataloader, device)\nprint(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:57:58.671609Z","iopub.execute_input":"2024-05-06T15:57:58.671962Z","iopub.status.idle":"2024-05-06T15:58:12.645712Z","shell.execute_reply.started":"2024-05-06T15:57:58.671933Z","shell.execute_reply":"2024-05-06T15:58:12.644738Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Test Loss: 0.0000, Test Accuracy: 1.0000\n","output_type":"stream"}]}]}